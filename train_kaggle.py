"""
–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º Kaggle –¥–∞—Ç–∞—Å–µ—Ç–µ (~45K –º–∞—Ç—á–µ–π).
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–≥—Ä–æ–∫–æ–≤ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ–∏—á–∏.
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, roc_auc_score, classification_report,
                           precision_score, recall_score, f1_score)
import warnings
warnings.filterwarnings('ignore')

try:
    from catboost import CatBoostClassifier
    HAS_CATBOOST = True
except ImportError:
    HAS_CATBOOST = False
    print("CatBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º GradientBoosting")


def load_data(data_dir='data/processed_kaggle'):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç train/val/test –¥–∞–Ω–Ω—ã–µ."""
    train = pd.read_csv(f'{data_dir}/train.csv')
    val = pd.read_csv(f'{data_dir}/val.csv')
    test = pd.read_csv(f'{data_dir}/test.csv')
    return train, val, test


def get_features():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏."""
    # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    base_features = [
        'rank_diff',
        'abs_rank_diff',
    ]
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–≥—Ä–æ–∫–æ–≤ (–∏–∑ Kaggle –¥–∞—Ç–∞—Å–µ—Ç–∞)
    player_stats = [
        'team_A_avg_rating',
        'team_A_avg_kd',
        'team_A_avg_adr',
        'team_A_avg_kast',
        'team_B_avg_rating',
        'team_B_avg_kd', 
        'team_B_avg_adr',
        'team_B_avg_kast',
    ]
    
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ–∏—á–∏ (–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ)
    advanced_features = [
        'elo_diff',
        'map_elo_diff',
        'h2h_rate',
        'h2h_games',
        'momentum_diff',
        'streak_A',
        'streak_B',
        'days_since_last_A',
        'days_since_last_B',
        'overall_winrate_A',
        'overall_winrate_B',
        'winrate_diff',
        'map_games_A',
        'map_games_B',
        'map_experience_diff',
    ]
    
    return base_features + player_stats + advanced_features


def add_derived_features(df):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏."""
    # –†–∞–∑–Ω–∏—Ü—ã –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –∏–≥—Ä–æ–∫–æ–≤
    if 'team_A_avg_rating' in df.columns:
        df['rating_diff'] = df['team_A_avg_rating'] - df['team_B_avg_rating']
        df['kd_diff'] = df['team_A_avg_kd'] - df['team_B_avg_kd']
        df['adr_diff'] = df['team_A_avg_adr'] - df['team_B_avg_adr']
        df['kast_diff'] = df['team_A_avg_kast'] - df['team_B_avg_kast']
    
    # Streak —Ä–∞–∑–Ω–∏—Ü–∞
    if 'streak_A' in df.columns:
        df['streak_diff'] = df['streak_A'] - df['streak_B']
    
    # –£—Å—Ç–∞–ª–æ—Å—Ç—å (–º–Ω–æ–≥–æ –∏–≥—Ä –ø–æ–¥—Ä—è–¥)
    if 'days_since_last_A' in df.columns:
        df['rest_diff'] = df['days_since_last_B'] - df['days_since_last_A']  # –±–æ–ª—å—à–µ = –ª—É—á—à–µ –æ—Ç–¥–æ—Ö–Ω—É–ª
    
    return df


def get_extended_features():
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–º–∏."""
    base = get_features()
    derived = [
        'rating_diff', 'kd_diff', 'adr_diff', 'kast_diff',
        'streak_diff', 'rest_diff'
    ]
    return base + derived


def prepare_data(train, val, test):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ —Ñ–∏—á–∏
    train = add_derived_features(train.copy())
    val = add_derived_features(val.copy())
    test = add_derived_features(test.copy())
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π
    all_features = get_extended_features()
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    available = [f for f in all_features if f in train.columns]
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(available)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(all_features)}")
    
    X_train = train[available].fillna(0)
    y_train = train['winner_is_A']
    
    X_val = val[available].fillna(0)
    y_val = val['winner_is_A']
    
    X_test = test[available].fillna(0)
    y_test = test['winner_is_A']
    
    return X_train, y_train, X_val, y_val, X_test, y_test, available


def train_and_evaluate():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π."""
    print("="*70)
    print("–¢–†–ï–ù–ò–†–û–í–ö–ê –ú–û–î–ï–õ–ò –ù–ê KAGGLE –î–ê–¢–ê–°–ï–¢–ï (45K+ –º–∞—Ç—á–µ–π)")
    print("="*70)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train, val, test = load_data()
    print(f"   Train: {len(train)}, Val: {len(val)}, Test: {len(test)}")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
    X_train, y_train, X_val, y_val, X_test, y_test, features = prepare_data(train, val, test)
    
    print(f"\nüìã –ü—Ä–∏–∑–Ω–∞–∫–∏ ({len(features)}):")
    for i, f in enumerate(features):
        print(f"   {i+1:2d}. {f}")
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = {}
    
    # --- CatBoost ---
    print("\n" + "-"*70)
    print("üöÄ CatBoost")
    print("-"*70)
    
    if HAS_CATBOOST:
        cb = CatBoostClassifier(
            iterations=1000,
            depth=6,
            learning_rate=0.05,
            l2_leaf_reg=3,
            random_seed=42,
            early_stopping_rounds=50,
            verbose=100
        )
        cb.fit(X_train, y_train, eval_set=(X_val, y_val))
        
        y_pred_cb = cb.predict(X_test)
        y_prob_cb = cb.predict_proba(X_test)[:, 1]
        
        acc_cb = accuracy_score(y_test, y_pred_cb)
        auc_cb = roc_auc_score(y_test, y_prob_cb)
        
        print(f"\n‚úÖ Test Accuracy: {acc_cb:.4f} ({acc_cb*100:.2f}%)")
        print(f"‚úÖ Test ROC-AUC:  {auc_cb:.4f}")
        
        results['CatBoost'] = {'accuracy': acc_cb, 'auc': auc_cb, 'model': cb, 'proba': y_prob_cb}
        
        # Feature importance
        print("\nüìä –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (CatBoost):")
        importance = cb.get_feature_importance()
        feat_imp = sorted(zip(features, importance), key=lambda x: -x[1])
        for name, imp in feat_imp[:15]:
            print(f"   {name:30s}: {imp:.2f}")
    
    # --- Random Forest ---
    print("\n" + "-"*70)
    print("üå≤ Random Forest")
    print("-"*70)
    
    rf = RandomForestClassifier(
        n_estimators=300,
        max_depth=12,
        min_samples_leaf=20,
        n_jobs=-1,
        random_state=42
    )
    rf.fit(X_train, y_train)
    
    y_pred_rf = rf.predict(X_test)
    y_prob_rf = rf.predict_proba(X_test)[:, 1]
    
    acc_rf = accuracy_score(y_test, y_pred_rf)
    auc_rf = roc_auc_score(y_test, y_prob_rf)
    
    print(f"‚úÖ Test Accuracy: {acc_rf:.4f} ({acc_rf*100:.2f}%)")
    print(f"‚úÖ Test ROC-AUC:  {auc_rf:.4f}")
    
    results['RandomForest'] = {'accuracy': acc_rf, 'auc': auc_rf, 'model': rf, 'proba': y_prob_rf}
    
    # --- Logistic Regression (baseline) ---
    print("\n" + "-"*70)
    print("üìà Logistic Regression (baseline)")
    print("-"*70)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    lr = LogisticRegression(max_iter=1000, random_state=42)
    lr.fit(X_train_scaled, y_train)
    
    y_pred_lr = lr.predict(X_test_scaled)
    y_prob_lr = lr.predict_proba(X_test_scaled)[:, 1]
    
    acc_lr = accuracy_score(y_test, y_pred_lr)
    auc_lr = roc_auc_score(y_test, y_prob_lr)
    
    print(f"‚úÖ Test Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)")
    print(f"‚úÖ Test ROC-AUC:  {auc_lr:.4f}")
    
    results['LogisticRegression'] = {'accuracy': acc_lr, 'auc': auc_lr}
    
    # --- –ê–Ω—Å–∞–º–±–ª—å ---
    print("\n" + "-"*70)
    print("üéØ –ê–Ω—Å–∞–º–±–ª—å (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π)")
    print("-"*70)
    
    if HAS_CATBOOST:
        y_prob_ensemble = (y_prob_cb + y_prob_rf) / 2
    else:
        y_prob_ensemble = y_prob_rf
    
    y_pred_ensemble = (y_prob_ensemble >= 0.5).astype(int)
    acc_ens = accuracy_score(y_test, y_pred_ensemble)
    auc_ens = roc_auc_score(y_test, y_prob_ensemble)
    
    print(f"‚úÖ Test Accuracy: {acc_ens:.4f} ({acc_ens*100:.2f}%)")
    print(f"‚úÖ Test ROC-AUC:  {auc_ens:.4f}")
    
    results['Ensemble'] = {'accuracy': acc_ens, 'auc': auc_ens, 'proba': y_prob_ensemble}
    
    # --- –ê–Ω–∞–ª–∏–∑ –ø–æ –ø–æ—Ä–æ–≥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ---
    print("\n" + "="*70)
    print("üìä –ê–ù–ê–õ–ò–ó –ü–û –ü–û–†–û–ì–£ –£–í–ï–†–ï–ù–ù–û–°–¢–ò")
    print("="*70)
    
    best_proba = y_prob_ensemble
    print(f"\n{'–ü–æ—Ä–æ–≥':>10} | {'Accuracy':>10} | {'–ü–æ–∫—Ä—ã—Ç–∏–µ':>10} | {'–ú–∞—Ç—á–µ–π':>10}")
    print("-"*50)
    
    confidence_results = []
    for threshold in [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80]:
        mask = (best_proba >= threshold) | (best_proba <= (1 - threshold))
        if mask.sum() > 0:
            acc_at_thresh = accuracy_score(y_test[mask], y_pred_ensemble[mask])
            coverage = mask.sum() / len(y_test)
            print(f"{threshold:>10.2f} | {acc_at_thresh:>10.4f} | {coverage:>10.2%} | {mask.sum():>10d}")
            confidence_results.append({
                'threshold': threshold,
                'accuracy': acc_at_thresh,
                'coverage': coverage,
                'count': mask.sum()
            })
    
    # --- –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ---
    print("\n" + "="*70)
    print("üìã –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*70)
    
    print(f"\n{'–ú–æ–¥–µ–ª—å':25s} | {'Accuracy':>10} | {'ROC-AUC':>10}")
    print("-"*50)
    for name, res in sorted(results.items(), key=lambda x: -x[1]['accuracy']):
        print(f"{name:25s} | {res['accuracy']:>10.4f} | {res['auc']:>10.4f}")
    
    # –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    best_model = max(results.items(), key=lambda x: x[1]['accuracy'])
    print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model[0]}")
    print(f"   Accuracy: {best_model[1]['accuracy']*100:.2f}%")
    print(f"   ROC-AUC: {best_model[1]['auc']:.4f}")
    
    # –í—ã–≤–æ–¥ –ø–æ confidence
    if confidence_results:
        best_conf = max(confidence_results, key=lambda x: x['accuracy'])
        print(f"\nüéØ –ü—Ä–∏ –ø–æ—Ä–æ–≥–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ {best_conf['threshold']:.2f}:")
        print(f"   Accuracy: {best_conf['accuracy']*100:.2f}%")
        print(f"   –ü–æ–∫—Ä—ã—Ç–∏–µ: {best_conf['coverage']*100:.1f}% –º–∞—Ç—á–µ–π ({best_conf['count']} —à—Ç)")
    
    return results


if __name__ == "__main__":
    results = train_and_evaluate()
